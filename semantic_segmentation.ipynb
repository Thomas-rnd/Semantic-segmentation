{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmAV1ohkct7o"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary\n",
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fz9Zl4K-V7H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"torchvision Version:\", torchvision.__version__)\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOyIDtm8-j6_"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdaZSGxrAlZW"
      },
      "outputs": [],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsXxNfwNa9Fr"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "\n",
        "class DeconvMobileNet(nn.Module):\n",
        "    def __init__(self, num_classes, init_weights):\n",
        "        super(DeconvMobileNet, self).__init__()\n",
        "\n",
        "        # Use the specified weights argument\n",
        "        mobilenet = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
        "        features = list(mobilenet.features.children())\n",
        "        classifier = list(mobilenet.classifier.children())\n",
        "\n",
        "        # Extracting layers from MobileNetV3\n",
        "        self.conv1 = nn.Sequential(features[0])\n",
        "        # self.pool1 = nn.MaxPool2d(kernel_size=1, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv2 = nn.Sequential(features[1], features[2], features[3])\n",
        "        # self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv3 = nn.Sequential(features[4], features[5], features[6])\n",
        "        # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv4 = nn.Sequential(features[7], features[8], features[9])\n",
        "        # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True) # stride 1 instead of 2\n",
        "\n",
        "        self.conv5 = nn.Sequential(features[10], features[11], features[12])\n",
        "        # self.pool5 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True) # stride 1 instead of 2\n",
        "\n",
        "        # Conv67 layer\n",
        "        self.conv67 = nn.Sequential(nn.Conv2d(576, 1024, kernel_size=(1, 1)),\n",
        "                                    nn.BatchNorm2d(1024),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(1024, 1000, kernel_size=(1, 1)),\n",
        "                                    nn.BatchNorm2d(1000),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        # Load weights for conv6 and conv7\n",
        "        w_conv6 = classifier[0].state_dict()\n",
        "        w_conv7 = classifier[3].state_dict()\n",
        "\n",
        "        new_input_channels_conv6 = 96\n",
        "        w_conv6_adjusted = w_conv6['weight'].unsqueeze(2).unsqueeze(3)  # .view(1024, 576, 1, 1)\n",
        "        self.conv67[0].weight.data.copy_(w_conv6_adjusted)\n",
        "        self.conv67[0].bias.data.copy_(w_conv6['bias'])\n",
        "        self.conv67[3].weight.data.copy_(w_conv7['weight'].view(1000, 1024, 1, 1))\n",
        "        self.conv67[3].bias.data.copy_(w_conv7['bias'])\n",
        "\n",
        "        # Define deconvolution layers\n",
        "        self.deconv67 = nn.Sequential(nn.ConvTranspose2d(1000, 1024, kernel_size=1, stride=1, padding=0),  # Mirrors conv67\n",
        "                                      nn.BatchNorm2d(1024),\n",
        "                                      nn.ReLU())\n",
        "\n",
        "        self.unpool5 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 576, kernel_size=3, stride=1, padding=1),  # Mirrors conv5\n",
        "            nn.BatchNorm2d(576),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(576, 576, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(576),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(576, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool4 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(96, 96, kernel_size=3, stride=1, padding=1),  # Mirrors conv4\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(96, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(96, 40, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(40, 40, kernel_size=3, stride=1, padding=1),  # Mirrors conv3\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(40, 40, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(40, 24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(24, 24, kernel_size=3, stride=1, padding=1),  # Mirrors conv2\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(24, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),  # Mirrors conv1\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        original = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        # x, p1 = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        # x, p2 = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        # x, p3 = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        # x, p4 = self.pool4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        # x, p5 = self.pool5(x)\n",
        "\n",
        "        x = self.conv67(x)\n",
        "        x = self.deconv67(x)\n",
        "\n",
        "        # x = self.unpool5(x)\n",
        "        x_interp = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.deconv5(x_interp)\n",
        "\n",
        "        # x = self.unpool4(x, p4)\n",
        "        x_interp = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.deconv4(x_interp)\n",
        "\n",
        "        # x = self.unpool3(x, p3)\n",
        "        x_interp = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.deconv3(x_interp)\n",
        "\n",
        "        # x = self.unpool2(x, p2)\n",
        "        x_interp = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.deconv2(x_interp)\n",
        "\n",
        "        # x = self.unpool1(x, p1)\n",
        "        x_interp = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.deconv1(x_interp)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        targets = [self.conv67, self.deconv67, self.deconv5, self.deconv4, self.deconv3, self.deconv2, self.deconv1]\n",
        "        for layer in targets:\n",
        "            for module in layer.children():\n",
        "                if isinstance(module, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(module.weight, 1)\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "                elif isinstance(module, nn.ConvTranspose2d):\n",
        "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                    if module.bias is not None:\n",
        "                        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Instantiate the model\n",
        "    num_classes = 21 # 20 classes + background\n",
        "    if torch.cuda.is_available():\n",
        "        test_model = DeconvMobileNet(num_classes=num_classes, init_weights=True).to('cuda')\n",
        "    else:\n",
        "        test_model = DeconvMobileNet(num_classes=num_classes, init_weights=True)\n",
        "\n",
        "    # Print model summary\n",
        "    input_shape = (3, 224, 224)  # Adjust the input shape accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        summary(test_model, input_size=input_shape, device='cuda')\n",
        "    else:\n",
        "        summary(test_model, input_size=input_shape, device='cpu')\n",
        "\n",
        "    # Generate random input data (batch size = 1, channels = 3, height = 224, width = 224)\n",
        "    input_shape = (1, 3, 224, 224)\n",
        "    if torch.cuda.is_available():\n",
        "        input_data = torch.randn(input_shape).to('cuda')\n",
        "    else:\n",
        "        input_data = torch.randn(input_shape)\n",
        "\n",
        "    # Forward pass\n",
        "    wanted_output_shape = (1, num_classes, 224, 224)\n",
        "    output = test_model(input_data)\n",
        "\n",
        "    # Print the output shape\n",
        "    assert output.size() == wanted_output_shape, f\"Current output Shape is {output.size()} and should be {wanted_output_shape}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBHhSt3ALCRF"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "\n",
        "class DeconvMobileUNet(nn.Module):\n",
        "    def __init__(self, num_classes, init_weights):\n",
        "        super(DeconvMobileUNet, self).__init__()\n",
        "\n",
        "        # Use the specified weights argument\n",
        "        mobilenet = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
        "        features = list(mobilenet.features.children())\n",
        "        classifier = list(mobilenet.classifier.children())\n",
        "\n",
        "        # Extracting layers from MobileNetV3\n",
        "        self.conv1 = nn.Sequential(features[0])\n",
        "        # self.pool1 = nn.MaxPool2d(kernel_size=1, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv2 = nn.Sequential(features[1], features[2], features[3])\n",
        "        # self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv3 = nn.Sequential(features[4], features[5], features[6])\n",
        "        # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True)# stride 1 instead of 2\n",
        "\n",
        "        self.conv4 = nn.Sequential(features[7], features[8], features[9])\n",
        "        # self.pool4 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True) # stride 1 instead of 2\n",
        "\n",
        "        self.conv5 = nn.Sequential(features[10], features[11], features[12])\n",
        "        # self.pool5 = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, return_indices=True) # stride 1 instead of 2\n",
        "\n",
        "        # Conv67 layer\n",
        "        self.conv67 = nn.Sequential(nn.Conv2d(576, 1024, kernel_size=(7, 7)),\n",
        "                                    nn.BatchNorm2d(1024),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(1024, 1000, kernel_size=(1, 1)),\n",
        "                                    nn.BatchNorm2d(1000),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        # Load weights for conv6 and conv7\n",
        "        w_conv6 = classifier[0].state_dict()\n",
        "        w_conv7 = classifier[3].state_dict()\n",
        "\n",
        "        new_input_channels_conv6 = 96\n",
        "        w_conv6_adjusted = w_conv6['weight'].unsqueeze(2).unsqueeze(3)  # .view(1024, 576, 1, 1)\n",
        "        self.conv67[0].weight.data.copy_(w_conv6_adjusted)\n",
        "        self.conv67[0].bias.data.copy_(w_conv6['bias'])\n",
        "        self.conv67[3].weight.data.copy_(w_conv7['weight'].view(1000, 1024, 1, 1))\n",
        "        self.conv67[3].bias.data.copy_(w_conv7['bias'])\n",
        "\n",
        "        # Define deconvolution layers\n",
        "        self.deconv67 = nn.Sequential(nn.ConvTranspose2d(1000, 576, kernel_size=7, stride=1, padding=0),  # Mirrors conv67\n",
        "                                      nn.BatchNorm2d(576),\n",
        "                                      nn.ReLU())\n",
        "\n",
        "        self.unpool5 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(576 + 576, 576, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(576),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(576, 576, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(576),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(576, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool4 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(96 + 96, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(96, 96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(96, 40, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(40 + 40, 40, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(40, 40, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(40, 24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(24 + 24, 24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(24, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(16 + 16, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, num_classes, kernel_size=1, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        original = x\n",
        "\n",
        "        cx1 = self.conv1(x)\n",
        "        # print(f\"conv 1: {cx1.shape}\")\n",
        "        cx2 = self.conv2(cx1)\n",
        "        # print(f\"conv 2: {cx2.shape}\")\n",
        "        cx3 = self.conv3(cx2)\n",
        "        # print(f\"conv 3: {cx3.shape}\")\n",
        "        cx4 = self.conv4(cx3)\n",
        "        # print(f\"conv 4: {cx4.shape}\")\n",
        "        cx5 = self.conv5(cx4)\n",
        "        # print(f\"conv 5: {cx5.shape}\")\n",
        "\n",
        "        cx67 = self.conv67(cx5)\n",
        "        # print(f\"conv 67: {cx67.shape}\")\n",
        "        dx67 = self.deconv67(cx67)\n",
        "        # print(f\"deconv 67: {dx67.shape}\")\n",
        "\n",
        "        # x_interp_5 = torch.nn.functional.interpolate(dx67, scale_factor=2, mode='nearest')\n",
        "        # print(f\"interp 5: {x_interp_5.shape}\")\n",
        "        x5 = torch.cat([dx67, cx5], dim=1)\n",
        "        dx5 = self.deconv5(x5)\n",
        "        # print(f\"deconv 5: {dx5.shape}\")\n",
        "\n",
        "        # x_interp_4 = torch.nn.functional.interpolate(dx5, scale_factor=2, mode='nearest')\n",
        "        # print(f\"interp 4: {x_interp_4.shape}\")\n",
        "        x4 = torch.cat([dx5, cx4], dim=1)\n",
        "        dx4 = self.deconv4(x4)\n",
        "        # print(f\"deconv 4: {dx4.shape}\")\n",
        "\n",
        "        x_interp_3 = torch.nn.functional.interpolate(dx4, scale_factor=2, mode='nearest')\n",
        "        # print(f\"interp 3: {x_interp_3.shape}\")\n",
        "        x3 = torch.cat([x_interp_3, cx3], dim=1)\n",
        "        dx3 = self.deconv3(x3)\n",
        "        # print(f\"deconv 3: {dx3.shape}\")\n",
        "\n",
        "        x_interp_2 = torch.nn.functional.interpolate(dx3, scale_factor=2, mode='nearest')\n",
        "        # print(f\"interp 2: {x_interp_2.shape}\")\n",
        "        x2 = torch.cat([x_interp_2, cx2], dim=1)\n",
        "        dx2 = self.deconv2(x2)\n",
        "        # print(f\"deconv 2: {dx2.shape}\")\n",
        "\n",
        "        x_interp_1 = torch.nn.functional.interpolate(dx2, scale_factor=4, mode='nearest')\n",
        "        # print(f\"interp 1: {x_interp_1.shape}\")\n",
        "        x1 = torch.cat([x_interp_1, cx1], dim=1)\n",
        "        dx1 = self.deconv1(x1)\n",
        "        # print(f\"deconv 1: {dx1.shape}\")\n",
        "\n",
        "        mask = torch.nn.functional.interpolate(dx1, scale_factor=2, mode='nearest')\n",
        "        # print(f\"mask: {mask.shape}\")\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        targets = [self.conv67, self.deconv67, self.deconv5, self.deconv4, self.deconv3, self.deconv2, self.deconv1]\n",
        "        for layer in targets:\n",
        "            for module in layer.children():\n",
        "                if isinstance(module, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(module.weight, 1)\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "                elif isinstance(module, nn.ConvTranspose2d):\n",
        "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                    if module.bias is not None:\n",
        "                        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Instantiate the model\n",
        "    num_classes = 21 # 20 classes + background\n",
        "    if torch.cuda.is_available():\n",
        "        test_model = DeconvMobileUNet(num_classes=num_classes, init_weights=True).to('cuda')\n",
        "    else:\n",
        "        test_model = DeconvMobileUNet(num_classes=num_classes, init_weights=True)\n",
        "\n",
        "    # Print model summary\n",
        "    input_shape = (3, 224, 224)  # Adjust the input shape accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        summary(test_model, input_size=input_shape, device='cuda')\n",
        "    else:\n",
        "        summary(test_model, input_size=input_shape, device='cpu')\n",
        "\n",
        "    # Generate random input data (batch size = 1, channels = 3, height = 224, width = 224)\n",
        "    input_shape = (3, 3, 224, 224)\n",
        "    if torch.cuda.is_available():\n",
        "        input_data = torch.randn(input_shape).to('cuda')\n",
        "    else:\n",
        "        input_data = torch.randn(input_shape)\n",
        "\n",
        "    # Forward pass\n",
        "    wanted_output_shape = (3, num_classes, 224, 224)\n",
        "    output = test_model(input_data)\n",
        "\n",
        "    # Print the output shape\n",
        "    assert output.size() == wanted_output_shape, f\"Current output Shape is {output.size()} and should be {wanted_output_shape}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui2wb691diNh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "def make_layers():\n",
        "    vgg16_bn = models.vgg16_bn(pretrained=True)\n",
        "    features = list(vgg16_bn.features.children())\n",
        "    classifier = list(vgg16_bn.classifier.children())\n",
        "\n",
        "    conv1 = nn.Sequential(*features[:6])\n",
        "    conv2 = nn.Sequential(*features[7:13])\n",
        "    conv3 = nn.Sequential(*features[14:23])\n",
        "    conv4 = nn.Sequential(*features[24:33])\n",
        "    conv5 = nn.Sequential(*features[34:43])\n",
        "\n",
        "    conv6 = nn.Conv2d(512, 4096, kernel_size=(7, 7))\n",
        "    conv7 = nn.Conv2d(4096, 4096, kernel_size=(1, 1))\n",
        "\n",
        "    w_conv6 = classifier[0].state_dict()\n",
        "    w_conv7 = classifier[3].state_dict()\n",
        "\n",
        "    conv6.load_state_dict({'weight':w_conv6['weight'].view(4096, 512, 7, 7), 'bias':w_conv6['bias']})\n",
        "    conv7.load_state_dict({'weight':w_conv7['weight'].view(4096, 4096, 1, 1), 'bias':w_conv7['bias']})\n",
        "\n",
        "    return [conv1, conv2, conv3, conv4, conv5, conv6, conv7]\n",
        "\n",
        "\n",
        "class DeconvNet(nn.Module):\n",
        "    def __init__(self, num_classes, init_weights):\n",
        "        super(DeconvNet, self).__init__()\n",
        "\n",
        "        layers = make_layers()\n",
        "\n",
        "        self.conv1 = layers[0]\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.conv2 = layers[1]\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.conv3 = layers[2]\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.conv4 = layers[3]\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.conv5 = layers[4]\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, return_indices=True)\n",
        "\n",
        "        self.conv67 = nn.Sequential(layers[5], nn.BatchNorm2d(4096), nn.ReLU(),\n",
        "                                    layers[6], nn.BatchNorm2d(4096), nn.ReLU())\n",
        "\n",
        "        self.deconv67 = nn.Sequential(nn.ConvTranspose2d(4096, 512, kernel_size=7, stride=1, padding=0), nn.BatchNorm2d(512), nn.ReLU())\n",
        "\n",
        "        self.unpool5 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU())\n",
        "\n",
        "        self.unpool4 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU())\n",
        "\n",
        "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU())\n",
        "\n",
        "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU())\n",
        "\n",
        "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        self.deconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, num_classes, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        original = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x, p1 = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x, p2 = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x, p3 = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x, p4 = self.pool4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x, p5 = self.pool5(x)\n",
        "\n",
        "\n",
        "        x = self.conv67(x)\n",
        "        x = self.deconv67(x)\n",
        "\n",
        "        x = self.unpool5(x, p5)\n",
        "        x = self.deconv5(x)\n",
        "\n",
        "        x = self.unpool4(x, p4)\n",
        "        x = self.deconv4(x)\n",
        "\n",
        "        x = self.unpool3(x, p3)\n",
        "        x = self.deconv3(x)\n",
        "\n",
        "        x = self.unpool2(x, p2)\n",
        "        x = self.deconv2(x)\n",
        "\n",
        "        x = self.unpool1(x, p1)\n",
        "        x = self.deconv1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        targets = [self.conv67, self.deconv67, self.deconv5, self.deconv4, self.deconv3, self.deconv2, self.deconv1]\n",
        "        for layer in targets:\n",
        "            for module in layer:\n",
        "                if isinstance(module, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(module.weight, 1)\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "                elif isinstance(module, nn.ConvTranspose2d):\n",
        "                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                    if module.bias is not None:\n",
        "                        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "model = DeconvNet(21, init_weights=True)\n",
        "input_shape = (3, 224, 224)\n",
        "summary(model, input_size=input_shape, device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6JXYEXoEd_Ej"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "from datetime import datetime\n",
        "import time\n",
        "from collections.abc import Sequence\n",
        "import os\n",
        "import random\n",
        "import numbers\n",
        "\n",
        "# Third-party Library Imports\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# PyTorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "import torchvision.transforms.v2.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P76x4cDept4S"
      },
      "outputs": [],
      "source": [
        "VOC_CLASSES = np.array([\n",
        "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\",\n",
        "    \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\",\n",
        "])\n",
        "\n",
        "VOC_COLORMAP = np.array([\n",
        "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128],\n",
        "    [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
        "    [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0],\n",
        "    [0, 192, 0], [128, 192, 0], [0, 64, 128],\n",
        "])\n",
        "\n",
        "class Mask_Aug:\n",
        "    \"\"\"\n",
        "    Class for applying a series of transformations to an image and its corresponding mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        Initializes the Mask_Aug instance with a list of transformations.\n",
        "\n",
        "        Args:\n",
        "        - transforms (list): List of transformation functions to be applied.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Applies the list of transformations to the input image and mask.\n",
        "\n",
        "        Args:\n",
        "        - image (PIL.Image.Image, mode='RGB'): Input image.\n",
        "        - mask (PIL.PngImagePlugin.PngImageFile, mode='P'): Input mask.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing transformed image and mask.\n",
        "        \"\"\"\n",
        "        for t in self.transforms:\n",
        "            image, mask = t(image, mask)\n",
        "        return image, mask\n",
        "\n",
        "class ToTensor:\n",
        "    \"\"\"\n",
        "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    Only applied to image not mask.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Converts the input image to a torch tensor.\n",
        "\n",
        "        Args:\n",
        "        - image (PIL.Image.Image, mode='RGB'): Input image.\n",
        "        - mask (PIL.PngImagePlugin.PngImageFile, mode='P'): Input mask.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing transformed image and original mask.\n",
        "        \"\"\"\n",
        "        return F.to_tensor(image), mask\n",
        "\n",
        "class PILToTensor:\n",
        "    \"\"\"\n",
        "    Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).\n",
        "    Only applied to mask not image.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Converts the input mask to a torch tensor.\n",
        "\n",
        "        Args:\n",
        "        - image (PIL.Image.Image, mode='RGB'): Input image.\n",
        "        - mask (PIL.PngImagePlugin.PngImageFile, mode='P'): Input mask.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing original image and transformed mask.\n",
        "        \"\"\"\n",
        "        mask = F.pil_to_tensor(mask)\n",
        "        # Remove border\n",
        "        # Or possibility to add ignore_index=255 to loss function\n",
        "        mask[mask == 255] = 0\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class ToPILImage:\n",
        "    \"\"\"\n",
        "    Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range.\n",
        "    \"\"\"\n",
        "    def __init__(self, mode=None):\n",
        "        \"\"\"\n",
        "        Initializes the ToPILImage instance with an optional mode for the PIL Image.\n",
        "\n",
        "        Args:\n",
        "        - mode (str, optional): The mode to use for PIL Image conversion.\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Converts the input torch tensor and mask to PIL Image.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing PIL Image representations of the image and colored mask.\n",
        "        \"\"\"\n",
        "        # Converting the mask tensor values into their corresponding RGB color values based on\n",
        "        # the mapping defined in VOC_COLORMAP.\n",
        "        colored_mask = VOC_COLORMAP[mask.cpu()].squeeze(0).astype('uint8')\n",
        "        return F.to_pil_image(image, self.mode), F.to_pil_image(colored_mask, self.mode)\n",
        "\n",
        "class ToDtype:\n",
        "    \"\"\"\n",
        "    Converts the datatype of the input image to the specified datatype.\n",
        "    \"\"\"\n",
        "    def __init__(self, dtype, scale=False):\n",
        "        \"\"\"\n",
        "        Initializes the ToDtype instance with a target datatype and an optional scale flag.\n",
        "\n",
        "        Args:\n",
        "        - dtype (torch.dtype): Target datatype.\n",
        "        - scale (bool, optional): Flag indicating whether to scale the image. Default is False.\n",
        "        \"\"\"\n",
        "        self.dtype = dtype\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Converts the datatype of the input image.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing transformed image and original mask.\n",
        "        \"\"\"\n",
        "        if not self.scale:\n",
        "            return image.to(dtype=self.dtype), mask\n",
        "        image = F.convert_image_dtype(image, self.dtype)\n",
        "        return image, mask\n",
        "\n",
        "class Normalize:\n",
        "    \"\"\"\n",
        "    Normalize a tensor image with mean and standard deviation.\n",
        "    Only applied to image not mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        \"\"\"\n",
        "        Initializes the Normalize instance with mean and standard deviation values.\n",
        "\n",
        "        Args:\n",
        "        - mean (list): List of mean values for each channel.\n",
        "        - std (list): List of standard deviation values for each channel.\n",
        "        \"\"\"\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Applies normalization to the input image.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing normalized image and original mask.\n",
        "        \"\"\"\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, mask\n",
        "\n",
        "class Resize(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Resize the input image to the given size.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, image_interpolation=Image.BILINEAR, mask_interpolation=Image.NEAREST, antialias=True):\n",
        "        \"\"\"\n",
        "        Initializes the Resize instance with size and interpolation options.\n",
        "\n",
        "        Args:\n",
        "        - size (int or tuple): Target size for resizing.\n",
        "        - image_interpolation (int, optional): Interpolation method for image resizing. Default is Image.BILINEAR.\n",
        "        - mask_interpolation (int, optional): Interpolation method for mask resizing. Default is Image.NEAREST.\n",
        "        - antialias (bool, optional): Flag indicating whether to use antialiasing. Default is True.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not isinstance(size, (int, Sequence)):\n",
        "            raise TypeError('Size should be int or sequence. Got {}'.format(type(size)))\n",
        "        if isinstance(size, Sequence) and len(size) not in (1, 2):\n",
        "            raise ValueError('If size is a sequence, it should have 1 or 2 values')\n",
        "        self.size = size\n",
        "        self.image_interpolation = image_interpolation\n",
        "        self.mask_interpolation = mask_interpolation\n",
        "        self.antialias = antialias\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        \"\"\"\n",
        "        Resizes the input image and mask.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing resized image and mask.\n",
        "        \"\"\"\n",
        "        return F.resize(image, self.size, self.image_interpolation, antialias=self.antialias), F.resize(mask, self.size, self.mask_interpolation, antialias=self.antialias)\n",
        "\n",
        "\n",
        "class RandomCrop(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Crop the given image at a random location.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_params(image, output_size):\n",
        "        \"\"\"\n",
        "        Returns random parameters for cropping.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - output_size (tuple): Desired output size.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing parameters (i, j, h, w) for cropping.\n",
        "        \"\"\"\n",
        "        h, w = F.get_size(image)\n",
        "        th, tw = output_size\n",
        "\n",
        "        if (h + 1 < th) or (w + 1 < tw):\n",
        "            raise ValueError('Required crop size {} is larger than input image size {}'.format((th, tw), (h, w)))\n",
        "\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "\n",
        "        i = torch.randint(0, h - th + 1, size=(1,)).item()\n",
        "        j = torch.randint(0, w - tw + 1, size=(1,)).item()\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'):\n",
        "        \"\"\"\n",
        "        Initializes the RandomCrop instance with parameters for cropping.\n",
        "\n",
        "        Args:\n",
        "        - size (tuple): Desired output size.\n",
        "        - padding (int or tuple, optional): Padding to be applied before cropping. Default is None.\n",
        "        - pad_if_needed (bool, optional): Flag indicating whether to pad if needed. Default is False.\n",
        "        - fill (int, optional): Pixel fill value for padding. Default is 0.\n",
        "        - padding_mode (str, optional): Padding mode. Default is 'constant'.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = tuple(_setup_size(\n",
        "            size, error_msg='Please provide only two dimensions (h, w) for size.'))\n",
        "        self.padding = padding\n",
        "        self.pad_if_needed = pad_if_needed\n",
        "        self.fill = fill\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        \"\"\"\n",
        "        Applies random cropping to the input image and mask.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing cropped image and mask.\n",
        "        \"\"\"\n",
        "        if self.padding is not None:\n",
        "            image = F.pad(image, self.padding, self.fill, self.padding_mode)\n",
        "            mask = F.pad(mask, self.padding, self.fill, self.padding_mode)\n",
        "\n",
        "        height, width = F.get_size(image)\n",
        "\n",
        "        # pad if needed\n",
        "        if self.pad_if_needed and (width < self.size[1]):\n",
        "            padding = [self.size[1] - width, 0]\n",
        "            image = F.pad(image, padding, self.fill, self.padding_mode)\n",
        "            mask = F.pad(mask, padding, self.fill, self.padding_mode)\n",
        "\n",
        "        if self.pad_if_needed and (height < self.size[0]):\n",
        "            padding = [0, self.size[0] - height]\n",
        "            image = F.pad(image, padding, self.fill, self.padding_mode)\n",
        "            mask = F.pad(mask, padding, self.fill, self.padding_mode)\n",
        "\n",
        "        i, j, h, w = self.get_params(image, self.size)\n",
        "\n",
        "        return F.crop(image, i, j, h, w), F.crop(mask, i, j, h, w)\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    \"\"\"\n",
        "    Randomly flip the input image and mask horizontally.\n",
        "    \"\"\"\n",
        "    def __init__(self, flip_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initializes the RandomHorizontalFlip instance with a flip probability.\n",
        "\n",
        "        Args:\n",
        "        - flip_prob (float, optional): Probability of horizontal flip.\n",
        "        \"\"\"\n",
        "        self.flip_prob = flip_prob\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        \"\"\"\n",
        "        Applies random horizontal flip to the input image and mask.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image tensor.\n",
        "        - mask (torch.Tensor): Input mask tensor.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing flipped image and mask.\n",
        "        \"\"\"\n",
        "        if random.random() < self.flip_prob:\n",
        "            image = F.hflip(image)\n",
        "            mask = F.hflip(mask)\n",
        "        return image, mask\n",
        "\n",
        "def _setup_size(size, error_msg):\n",
        "    \"\"\"\n",
        "    Helper function to validate and process the size argument.\n",
        "\n",
        "    Args:\n",
        "    - size: Size argument to be processed.\n",
        "    - error_msg (str): Error message to be used in case of validation failure.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Tuple containing processed size.\n",
        "    \"\"\"\n",
        "    if isinstance(size, numbers.Number):\n",
        "        return int(size), int(size)\n",
        "\n",
        "    if isinstance(size, Sequence) and len(size) == 1:\n",
        "        return size[0], size[0]\n",
        "\n",
        "    if len(size) != 2:\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mZ6fQf2ghAUk"
      },
      "outputs": [],
      "source": [
        "class SemanticSegmentationDataset:\n",
        "    \"\"\"\n",
        "    Custom dataset for semantic segmentation using the VOC dataset.\n",
        "\n",
        "    Args:\n",
        "    - root (str): Root directory of the VOC Dataset.\n",
        "    - year (str): The dataset year, supports years \"2007\" to \"2012\".\n",
        "    - image_set (str): Select the image_set to use.\n",
        "    - batch_size (int, optional): Batch size for DataLoader. Default is 64.\n",
        "    - shuffle (bool, optional): If True, shuffles the dataset. Default is True.\n",
        "    - transforms (callable, optional): Transforms to be applied to the dataset.\n",
        "\n",
        "    Attributes:\n",
        "    - batch_size (int): Batch size for DataLoader.\n",
        "    - transforms (callable): Transforms applied to the dataset.\n",
        "    - dataset: VOC Segmentation dataset instance.\n",
        "    - loader: DataLoader instance for the dataset.\n",
        "\n",
        "    Raises:\n",
        "    - AssertionError: If input and target shapes do not match the expected shapes.\n",
        "    - AssertionError: If target values are not within the specified range.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, year, image_set, batch_size=64, shuffle=True, transforms=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.dataset = VOCSegmentation(root=root,\n",
        "                                       year=year,\n",
        "                                       image_set=image_set,\n",
        "                                       download=True,\n",
        "                                       transforms=self.transforms)\n",
        "\n",
        "        self.loader = DataLoader(self.dataset,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 shuffle=shuffle,\n",
        "                                 pin_memory=True)\n",
        "\n",
        "        # Asserts\n",
        "        input_shape = (self.batch_size, 3, 224, 224)\n",
        "        target_shape = (self.batch_size, 1, 224, 224)\n",
        "        images, targets = next(iter(self.loader))\n",
        "\n",
        "        assert images.size() == input_shape, f\"Wrong input shape, found {images.size()}, expected {input_shape}\"\n",
        "        assert targets.size() == target_shape, f\"Wrong target shape, found {targets.size()}, expected {target_shape}\"\n",
        "\n",
        "        # Verify that all the first element of the first batch verify assert\n",
        "        mask = targets[0]\n",
        "        mask_np = mask.numpy()\n",
        "        unique_values = np.unique(mask_np)\n",
        "\n",
        "        assert all(0 <= value < num_classes for value in unique_values), \"Values are not within the specified range.\"\n",
        "\n",
        "    def plot(self, model):\n",
        "        \"\"\"\n",
        "        Plots input image, ground truth, and prediction for a given model.\n",
        "\n",
        "        Args:\n",
        "        - model: Semantic segmentation model.\n",
        "        \"\"\"\n",
        "        # Consider only for batch size 1 on test_data\n",
        "        iter_batch = iter(self.loader)\n",
        "\n",
        "        for idx in range(3):\n",
        "            image, target = next(iter_batch)\n",
        "            if torch.cuda.is_available():\n",
        "                image, target = image.cuda(gpu_id, non_blocking=True), target.cuda(gpu_id, non_blocking=True)\n",
        "            prediction = model(image)\n",
        "\n",
        "            # Display the image, ground truth, and prediction\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            input_image, ground_truth = ToPILImage()(image.squeeze(0), target.squeeze(0))\n",
        "            predicted_mask = prediction.argmax(dim=1).byte()\n",
        "            c, h, w = predicted_mask.shape\n",
        "            _, predicted_mask = ToPILImage()(image.squeeze(0), predicted_mask)\n",
        "\n",
        "            # Input Image\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(input_image)\n",
        "            plt.title('Input Image')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Ground Truth\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(ground_truth)\n",
        "            plt.title('Ground Truth')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Prediction\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(predicted_mask)\n",
        "            plt.title('Prediction')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lXgJJhIbhPfC"
      },
      "outputs": [],
      "source": [
        "class SemanticSegmentationTrainer:\n",
        "    \"\"\"\n",
        "    Trainer class for semantic segmentation models.\n",
        "\n",
        "    Args:\n",
        "    - model: Semantic segmentation model.\n",
        "    - train_dataset: Training dataset instance.\n",
        "    - val_dataset: Validation dataset instance.\n",
        "    - criterion: Loss criterion.\n",
        "    - optimizer: Model optimizer.\n",
        "    - model_directory (str): Directory to save the model and optimizer checkpoints.\n",
        "    - num_epochs (int, optional): Number of training epochs. Default is 10.\n",
        "    - patience (int, optional): Patience for early stopping. Default is 500.\n",
        "    - gpu_id (int, optional): GPU device ID. Default is 0.\n",
        "\n",
        "    Attributes:\n",
        "    - model: Semantic segmentation model.\n",
        "    - model_is_best (bool): Flag indicating if the model is the best so far.\n",
        "    - train_dataset: Training dataset instance.\n",
        "    - val_dataset: Validation dataset instance.\n",
        "    - criterion: Loss criterion.\n",
        "    - optimizer: Model optimizer.\n",
        "    - model_directory (str): Directory to save the model and optimizer checkpoints.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "    - patience (int): Patience for early stopping.\n",
        "    - early_stopping_counter (int): Counter for early stopping.\n",
        "    - best_val_loss (float): Best validation loss.\n",
        "    - eps (float): Small value to prevent division by zero.\n",
        "    - gpu (int): GPU device ID.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_dataset, val_dataset, criterion, optimizer, model_directory, num_epochs=10, patience=500, gpu_id=0):\n",
        "        self.model = model\n",
        "        self.model_is_best = False\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.model_directory = model_directory\n",
        "        self.num_epochs = num_epochs\n",
        "        self.patience = patience\n",
        "        self.early_stopping_counter = 0\n",
        "        self.best_mIoU_loss = float('inf')\n",
        "        self.eps = 1e-10  # prevent division by 0\n",
        "        self.gpu = gpu_id\n",
        "\n",
        "    def save_model_and_optimizer(self, epoch, train_loss, val_loss, mIoU):\n",
        "        \"\"\"\n",
        "        Save model and optimizer checkpoint and log to WandB.\n",
        "\n",
        "        Args:\n",
        "        - epoch (int): Current training epoch.\n",
        "        - train_loss (float): Training loss.\n",
        "        - val_loss (float): Validation loss.\n",
        "        - mIoU (float): Mean Intersection over Union.\n",
        "        \"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'training_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'mIoU': mIoU\n",
        "        }\n",
        "\n",
        "        model_path = f\"{self.model_directory}/checkpoint_{epoch}.pth\"\n",
        "        torch.save(checkpoint, model_path)\n",
        "\n",
        "        # Create a Model Version in WandB\n",
        "        art = wandb.Artifact(f\"mobileNet-ss-{wandb.run.id}\", type=\"model\")\n",
        "        art.add_file(model_path, \"model.pth\")\n",
        "        # Log the Version\n",
        "        wandb.log_artifact(art, aliases=[\"latest\", \"best\"])\n",
        "\n",
        "        print(f'Model and optimizer saved at epoch {epoch} with validation loss {val_loss}, mIoU {mIoU}')\n",
        "\n",
        "    def mIoU(self, output, y):\n",
        "        \"\"\"\n",
        "        Calculate Mean Intersection over Union (mIoU).\n",
        "\n",
        "        Args:\n",
        "        - output: Model output.\n",
        "        - y: Ground truth.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: Tuple containing true positives, false positives, and false negatives.\n",
        "        \"\"\"\n",
        "        # only consider for batch size 1 on val_data\n",
        "        predict = torch.argmax(output, dim=1)\n",
        "        if torch.cuda.is_available():\n",
        "            true_positive = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "            false_positive = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "            false_negative = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "        else:\n",
        "            true_positive = torch.zeros(num_classes)\n",
        "            false_positive = torch.zeros(num_classes)\n",
        "            false_negative = torch.zeros(num_classes)\n",
        "\n",
        "        for i in range(num_classes):\n",
        "            positive_i = predict == i\n",
        "            true_i = y == i\n",
        "            true_positive[i] += torch.sum(torch.logical_and(positive_i, true_i))\n",
        "            false_positive[i] += torch.sum(torch.logical_and(positive_i, ~true_i))\n",
        "            false_negative[i] += torch.sum(torch.logical_and(~positive_i, true_i))\n",
        "\n",
        "        return true_positive, false_positive, false_negative\n",
        "\n",
        "    def early_stopping(self, mIoU):\n",
        "        \"\"\"\n",
        "        Check for early stopping based on mIoU loss.\n",
        "\n",
        "        Args:\n",
        "        - mIoU (float): Current mIoU loss.\n",
        "\n",
        "        Returns:\n",
        "        - bool: True if early stopping criteria met, False otherwise.\n",
        "        \"\"\"\n",
        "        if mIoU < self.best_mIoU_loss:\n",
        "            self.model_is_best = True\n",
        "            self.best_mIoU_loss = mIoU\n",
        "            self.early_stopping_counter = 0\n",
        "        else:\n",
        "            self.model_is_best = False\n",
        "            self.early_stopping_counter += 1\n",
        "\n",
        "        return self.early_stopping_counter >= self.patience\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop.\"\"\"\n",
        "        for epoch in range(self.num_epochs):\n",
        "            start_training_time = time.time()  # Record start time for the epoch\n",
        "            self.model.train()\n",
        "            losses = list()\n",
        "\n",
        "            for images, targets in self.train_dataset.loader:\n",
        "                batch, channel, height, width = targets.shape\n",
        "                # the target tensor should have the shape (batch_size, height, width)\n",
        "                targets = targets.view(batch, height, width).type(torch.LongTensor)\n",
        "\n",
        "                # Move the tensors to the GPU\n",
        "                if torch.cuda.is_available():\n",
        "                    images, targets = images.cuda(self.gpu, non_blocking=True), targets.cuda(self.gpu, non_blocking=True)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            avg_training_loss = sum(losses) / len(losses)\n",
        "            # Compute time per epoch\n",
        "            elapsed_training_time = time.time() - start_training_time\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            losses = list()\n",
        "            if torch.cuda.is_available():\n",
        "                true_positives = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "                false_positives = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "                false_negatives = torch.zeros(num_classes).cuda(self.gpu, non_blocking=True)\n",
        "            else:\n",
        "                true_positives = torch.zeros(num_classes)\n",
        "                false_positives = torch.zeros(num_classes)\n",
        "                false_negatives = torch.zeros(num_classes)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_images, val_targets in self.val_dataset.loader:\n",
        "                    batch, channel, height, width = val_targets.shape\n",
        "                    # the target tensor should have the shape (batch_size, height, width)\n",
        "                    val_targets = val_targets.view(batch, height, width).type(torch.LongTensor)\n",
        "\n",
        "                    # Move the tensors to the GPU\n",
        "                    if torch.cuda.is_available():\n",
        "                        val_images, val_targets = val_images.cuda(self.gpu, non_blocking=True), val_targets.cuda(self.gpu, non_blocking=True)\n",
        "\n",
        "                    val_outputs = self.model(val_images)\n",
        "                    val_loss = self.criterion(val_outputs, val_targets)\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                    tp, fp, fn = self.mIoU(val_outputs, val_targets)\n",
        "                    true_positives += tp\n",
        "                    false_positives += fp\n",
        "                    false_negatives += fn\n",
        "\n",
        "            mIoU = torch.sum(true_positives / (self.eps + true_positives + false_positives + false_negatives)) / num_classes\n",
        "            avg_val_loss = sum(losses) / len(losses)\n",
        "\n",
        "            print(f\"Epoch: {epoch}, training_time: {elapsed_training_time}, train_loss: {avg_training_loss}, val_loss: {avg_val_loss}, mIoU: {mIoU.item()}\")\n",
        "            wandb.log({\"epoch\": epoch, \"training_time\": elapsed_training_time, \"train_loss\": avg_training_loss, \"val_loss\": avg_val_loss, \"mIoU\": mIoU.item()})\n",
        "\n",
        "            # Early stopping check\n",
        "            if self.early_stopping(mIoU.item()):\n",
        "                print(f'Early stopping at epoch {epoch} due to no improvement in mIoU loss.')\n",
        "                break\n",
        "\n",
        "            # Save checkpoint\n",
        "            if self.model_is_best:\n",
        "                self.save_model_and_optimizer(epoch, avg_training_loss, avg_val_loss, mIoU.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c__az0ShOWw"
      },
      "outputs": [],
      "source": [
        "def load_model_and_optimizer(model, optimizer, path=\"checkpoint_{idx_epoch}.pth\"):\n",
        "    if torch.cuda.is_available():\n",
        "        checkpoint = torch.load(path)\n",
        "    else:\n",
        "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    training_loss = checkpoint['training_loss']\n",
        "    val_loss = checkpoint['val_loss']\n",
        "    mIoU = checkpoint['mIoU']\n",
        "\n",
        "    print(f'Model and optimizer loaded from epoch {epoch} with validation loss {val_loss}, mIoU {mIoU}')\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "class SemanticSegmentationPipeline:\n",
        "    \"\"\"\n",
        "    Semantic segmentation pipeline for training and evaluation.\n",
        "\n",
        "    Args:\n",
        "    - num_classes (int): Number of classes in the segmentation task.\n",
        "    - train_batch_size (int): Batch size for training.\n",
        "    - val_batch_size (int): Batch size for validation.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "    - root (str): Root directory of the VOC Dataset. Default is './data'.\n",
        "    - year (str): The dataset year, supports years \"2007\" to \"2012\". Default is '2012'.\n",
        "    - gpu_id (int): GPU device ID. Default is 0.\n",
        "\n",
        "    Attributes:\n",
        "    - root (str): Root directory of the VOC Dataset.\n",
        "    - year (str): The dataset year.\n",
        "    - num_classes (int): Number of classes in the segmentation task.\n",
        "    - train_batch_size (int): Batch size for training.\n",
        "    - val_batch_size (int): Batch size for validation.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "    - gpu (int): GPU device ID.\n",
        "\n",
        "    Methods:\n",
        "    - run(): Executes the semantic segmentation pipeline.\n",
        "\n",
        "    Raises:\n",
        "    - AssertionError: If the GPU device ID is invalid.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, train_batch_size, val_batch_size, num_epochs, root='./data', year='2012', gpu_id=0):\n",
        "        self.root = root\n",
        "        self.year = year\n",
        "        self.num_classes = num_classes\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        # Set GPU device\n",
        "        if torch.cuda.is_available():\n",
        "            assert 0 <= gpu_id < torch.cuda.device_count(), f\"Invalid GPU device ID {gpu_id}.\"\n",
        "            self.gpu = gpu_id\n",
        "            torch.cuda.set_device(self.gpu)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Execute the semantic segmentation pipeline.\n",
        "        Initializes WandB, sets up data transformations, datasets, model, loss, optimizer, and trainer.\n",
        "        Trains the model and logs results.\n",
        "        \"\"\"\n",
        "        wandb.init(\n",
        "            project=\"semantic-segmentation\",\n",
        "            config={\n",
        "                \"num_classes\": self.num_classes,\n",
        "                \"num_epochs\": self.num_epochs,\n",
        "                \"learning_rate\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"train_batch_size\": self.train_batch_size,\n",
        "                \"val_batch_size\": self.val_batch_size,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Define data transforms\n",
        "        train_tf = Mask_Aug(transforms=[ToTensor(),\n",
        "                                        PILToTensor(),\n",
        "                                        Resize((256, 256)), RandomCrop((224, 224)),\n",
        "                                        RandomHorizontalFlip(0.5),\n",
        "                                        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        val_tf = Mask_Aug(transforms=[ToTensor(),\n",
        "                                      PILToTensor(),\n",
        "                                      Resize((224, 224)),\n",
        "                                      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "        # Create training and validation datasets\n",
        "        train_dataset = SemanticSegmentationDataset(root=self.root,\n",
        "                                                    year=self.year,\n",
        "                                                    image_set='train',\n",
        "                                                    batch_size=self.train_batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    transforms=train_tf)\n",
        "\n",
        "        val_dataset = SemanticSegmentationDataset(root=self.root,\n",
        "                                                  year=self.year,\n",
        "                                                  image_set='val',\n",
        "                                                  batch_size=self.val_batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  transforms=val_tf)\n",
        "\n",
        "        # Create the segmentation model\n",
        "        if torch.cuda.is_available():\n",
        "            model = DeconvMobileNet(num_classes=self.num_classes, init_weights=True).cuda(self.gpu)\n",
        "            # model = DeconvMobileUNet(num_classes=self.num_classes, init_weights=True).cuda(self.gpu)\n",
        "        else:\n",
        "            model = DeconvMobileNet(num_classes=self.num_classes, init_weights=True)\n",
        "            # model = DeconvMobileUNet(num_classes=self.num_classes, init_weights=True)\n",
        "\n",
        "        # Define loss and optimizer\n",
        "        if torch.cuda.is_available():\n",
        "            criterion = nn.CrossEntropyLoss().cuda(self.gpu)\n",
        "        else:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=lr,\n",
        "                              momentum=momentum,\n",
        "                              weight_decay=weight_decay)\n",
        "\n",
        "        # optimizer = optim.AdamW(model.parameters(),\n",
        "        #          lr=lr,\n",
        "        #          weight_decay=weight_decay)\n",
        "\n",
        "        # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "\n",
        "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "\n",
        "        # Load previous best model and optimizer state\n",
        "        # model, optimizer = load_model_and_optimizer(model,\n",
        "        #                                           optimizer,\n",
        "        #                                            path='/content/gdrive/My Drive/semantic_segmentation/checkpoint/2023-12-09 08:27/checkpoint_73.pth')\n",
        "\n",
        "        # Create trainer and start training\n",
        "        trainer = SemanticSegmentationTrainer(model,\n",
        "                                              train_dataset,\n",
        "                                              val_dataset,\n",
        "                                              criterion,\n",
        "                                              optimizer,\n",
        "                                              model_directory,\n",
        "                                              self.num_epochs)\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # Visualize validation results\n",
        "        val_dataset.plot(model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    # Provide the path to Google Drive directory\n",
        "    drive_path = '/content/gdrive/My Drive/semantic_segmentation'\n",
        "\n",
        "    # Create the directory to save/load model\n",
        "    current_time = datetime.now()\n",
        "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M\")\n",
        "    model_directory = drive_path + '/checkpoint/' + formatted_time\n",
        "    os.makedirs(model_directory, exist_ok=True)\n",
        "\n",
        "    gpu_id = 0\n",
        "    num_classes = 21  # 20 classes + background\n",
        "    num_epochs = 90\n",
        "    lr = 0.01\n",
        "    momentum = 0.9\n",
        "    weight_decay = 0.0005\n",
        "    train_batch_size = 12\n",
        "    val_batch_size = 1 # Must be one\n",
        "\n",
        "    wandb.login()\n",
        "    pipeline = SemanticSegmentationPipeline(num_classes, train_batch_size, val_batch_size, num_epochs, gpu_id=gpu_id)\n",
        "    pipeline.run()\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syXoUloWfJz5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8SrYk-lFljo"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y47KIwTBFk-w"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def load_model_and_optimizer(model, optimizer, path=\"checkpoint_{idx_epoch}.pth\"):\n",
        "    if torch.cuda.is_available():\n",
        "        checkpoint = torch.load(path)\n",
        "    else:\n",
        "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    training_loss = checkpoint['training_loss']\n",
        "    val_loss = checkpoint['val_loss']\n",
        "    mIoU = checkpoint['mIoU']\n",
        "\n",
        "    print(f'Model and optimizer loaded from epoch {epoch} with validation loss {val_loss}, mIoU {mIoU}')\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = DeconvMobileNet(num_classes=21, init_weights=True).cuda(0)\n",
        "else:\n",
        "    model = DeconvMobileNet(num_classes=21, init_weights=True)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "model, _ = load_model_and_optimizer(model, optimizer, path='/content/gdrive/My Drive/semantic_segmentation/checkpoint/2023-12-09 08:27/checkpoint_73.pth')\n",
        "\n",
        "val_tf = Mask_Aug(transforms=[ToTensor(),\n",
        "                                      PILToTensor(),\n",
        "                                      Resize((224, 224)),\n",
        "                                      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "path_to_data = '/content/gdrive/My Drive/semantic_segmentation/data'\n",
        "val_dataset = SemanticSegmentationDataset(root='./data',\n",
        "                                          year='2012',\n",
        "                                          image_set='val',\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=False,\n",
        "                                          transforms=val_tf)\n",
        "\n",
        "#image_path = \"/content/gdrive/My Drive/semantic_segmentation/data/VOCdevkit/VOC2012/JPEGImages/2008_004917.jpg\"\n",
        "#original_image = Image.open(image_path)\n",
        "#input, _ = val_tf(original_image, original_image)\n",
        "\n",
        "val_dataset.plot(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}